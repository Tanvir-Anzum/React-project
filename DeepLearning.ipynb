{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tanvir-Anzum/React-project/blob/main/DeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYWAhavtATSS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMxq9QOireJl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_I9tg0GqT4G"
      },
      "source": [
        "# New section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WHtZhVNdTz3",
        "outputId": "022e845e-59c6-46f1-8349-471feef00ec9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzuTUl3uqmER"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9p1yTFIrtI5"
      },
      "outputs": [],
      "source": [
        "#importing necissary libraries\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(101)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "import os\n",
        "from os import listdir\n",
        "import cv2\n",
        "\n",
        "import imageio\n",
        "import skimage\n",
        "import skimage.io\n",
        "import skimage.transform\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage.io import imread\n",
        "import itertools\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gR98nwHNwiSa"
      },
      "outputs": [],
      "source": [
        "training_data = '/content/drive/MyDrive/Deep Learning/BreaKHis 400X/teain'\n",
        "\n",
        "test_data = '/content/drive/MyDrive/Deep Learning/BreaKHis 400X/test'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eviiCLGLzd1H"
      },
      "outputs": [],
      "source": [
        "# Pytorch tlements\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "# PyTorch TensorBoard support\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAv3lfLe3h-u",
        "outputId": "3505f472-f35e-4ae5-e3b6-5e67cfd0fc4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "gpu_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(gpu_device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNv0ieaP4xXK"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "# set the directory path\n",
        "dir_path = \"/content/drive/MyDrive/Deep Learning/BreaKHis 400X\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhHAMtw09JD6",
        "outputId": "7b280c96-b57e-40f0-dbac-db7ad7a9c87c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of augmented dataset: 12639\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "from torchvision import transforms\n",
        "import os\n",
        "\n",
        "transform = transforms.Compose([\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5,), (0.5,)),\n",
        "     transforms.Resize((246, 246)),\n",
        "])\n",
        "augmented_datasets = [ ]\n",
        "\n",
        "# Create datasets for training. and validation\n",
        "training_set = torchvision.datasets.ImageFolder('/content/drive/MyDrive/Deep Learning/BreaKHis 400X/train', transform=transform)\n",
        "test_set = torchvision.datasets.ImageFolder('/content/drive/MyDrive/Deep Learning/BreaKHis 400X/test', transform=transform)\n",
        "\n",
        "original_train_set = torchvision.datasets.ImageFolder('/content/drive/MyDrive/Deep Learning/BreaKHis 400X/train', transform=transform)\n",
        "\n",
        "\n",
        "# Define the number of times to augment the dataset\n",
        "num_augmentations = 10\n",
        "\n",
        "# Define the transforms for data augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(246),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    transforms.Resize(246)  # Add the missing resize transformation\n",
        "])\n",
        "\n",
        "for _ in range(num_augmentations):\n",
        "     # Apply the data augmentation to the original dataset\n",
        "     augmented_train_set = torchvision.datasets.ImageFolder('/content/drive/MyDrive/Deep Learning/BreaKHis 400X/train', transform=train_transform)\n",
        "\n",
        "     # Add the augmented dataset to the li\n",
        "     augmented_datasets.append(augmented_train_set)\n",
        "\n",
        "# Concatenate the augmented datasets\n",
        "augmented_train_set = ConcatDataset([original_train_set] + augmented_datasets)\n",
        "\n",
        "# Print the size of the augmented dataset\n",
        "print(\"Size of augmented dataset:\", len(augmented_train_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzaxCxsCFrAz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jKHaVzkg0sW"
      },
      "outputs": [],
      "source": [
        "# import torchvision.transforms as transforms\n",
        "\n",
        "# # Load the image tensor from the dataset\n",
        "# img_tensor, label = training_set[1]\n",
        "\n",
        "# directory = '/content/drive/MyDrive/My Graphs/'\n",
        "# path = '/content/drive/MyDrive/My Graphs/ka.png'\n",
        "\n",
        "# if not os.path.exists(path):\n",
        "#     os.makedirs(path)\n",
        "# if not os.path.exists(directory):\n",
        "#     os.makedirs(directory)\n",
        "\n",
        "# # Define the data augmentation transformations\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.RandomResizedCrop(224),\n",
        "#     transforms.RandomHorizontalFlip(),\n",
        "#     transforms.RandomRotation(10),\n",
        "#     transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "# ])\n",
        "\n",
        "# # Apply the transformations to the image tensor\n",
        "# augmented_img_tensor = transform(img_tensor)\n",
        "\n",
        "# # Convert the tensor back to a PIL image for visualization\n",
        "# augmented_img = transforms.ToPILImage()(augmented_img_tensor)\n",
        "\n",
        "# # Show the original and augmented images side-by-side\n",
        "# import matplotlib.pyplot as plt\n",
        "# fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
        "# axs[0].imshow(img_tensor.permute(1, 2, 0))\n",
        "# axs[0].set_title('Original')\n",
        "# axs[1].imshow(augmented_img)\n",
        "# axs[1].set_title('Augmented')\n",
        "# plt.savefig(path + 'ka.png')\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0U6vl9TK91A0"
      },
      "outputs": [],
      "source": [
        "batch_size_train=128\n",
        "batch_size_val=32\n",
        "batch_size_test=32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSFdbm0lid3C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbibD47SwfoX"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "train_size = int(len(augmented_train_set) * 0.8)\n",
        "val_size = len(augmented_train_set) - train_size\n",
        "\n",
        "# Split training set into training and validation sets\n",
        "train_set, val_set = random_split(augmented_train_set, [train_size, val_size])\n",
        "\n",
        "# Create data loaders for training, validation, and test sets\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size_train, shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size_val, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size_test, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J2PiCq1-zeH",
        "outputId": "f5763f50-efc6-4a72-eadf-58dcc9ab968f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set has 10111 instances\n",
            "Test set has 545 instances\n"
          ]
        }
      ],
      "source": [
        "classes = ('benign', 'malignant')\n",
        "\n",
        "# Report split sizes\n",
        "print('Training set has {} instances'.format(len(train_set)))\n",
        "print('Test set has {} instances'.format(len(test_set)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VRBGZx14UQk"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkWbhv4ibVZZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEVDQ0gl4Ws3",
        "outputId": "d4a0f2eb-d478-48c9-c521-8d4ee6f90b60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[4.7848e-02, 5.1845e-01],\n",
            "        [1.7851e-01, 6.1248e-01],\n",
            "        [8.7864e-01, 9.9556e-01],\n",
            "        [5.0684e-01, 9.4200e-01],\n",
            "        [6.9540e-02, 7.1553e-01],\n",
            "        [4.5648e-02, 1.7867e-01],\n",
            "        [5.7181e-01, 6.4342e-01],\n",
            "        [5.4594e-01, 5.0837e-02],\n",
            "        [1.9421e-01, 4.0214e-01],\n",
            "        [4.4397e-01, 9.0029e-01],\n",
            "        [2.5563e-01, 4.2380e-01],\n",
            "        [9.5598e-01, 5.5512e-01],\n",
            "        [8.3341e-02, 9.4308e-01],\n",
            "        [9.5607e-01, 9.0120e-01],\n",
            "        [7.6172e-03, 9.5079e-01],\n",
            "        [1.0341e-02, 9.9554e-01],\n",
            "        [5.4192e-01, 4.9333e-01],\n",
            "        [8.6872e-01, 7.3926e-01],\n",
            "        [9.7807e-01, 7.3217e-01],\n",
            "        [7.6662e-01, 8.0082e-02],\n",
            "        [4.3440e-01, 9.0700e-01],\n",
            "        [6.2786e-01, 5.8752e-01],\n",
            "        [8.3885e-01, 6.2316e-01],\n",
            "        [6.2161e-01, 3.0022e-01],\n",
            "        [5.3875e-01, 5.2412e-01],\n",
            "        [8.3599e-01, 7.3160e-01],\n",
            "        [3.5734e-01, 1.9525e-01],\n",
            "        [3.0747e-01, 8.3986e-01],\n",
            "        [7.8130e-01, 7.6854e-01],\n",
            "        [5.9526e-01, 3.5035e-01],\n",
            "        [3.6757e-01, 7.1990e-01],\n",
            "        [4.4663e-01, 9.2467e-01],\n",
            "        [2.1921e-01, 7.9379e-01],\n",
            "        [5.5686e-01, 7.0494e-01],\n",
            "        [6.9570e-01, 6.1603e-01],\n",
            "        [9.7437e-01, 5.2854e-01],\n",
            "        [2.7016e-01, 2.9849e-01],\n",
            "        [8.6456e-01, 5.1899e-01],\n",
            "        [6.2730e-01, 1.3281e-01],\n",
            "        [3.5239e-01, 9.6807e-01],\n",
            "        [6.2724e-01, 9.6326e-01],\n",
            "        [7.9429e-01, 4.2017e-01],\n",
            "        [2.4709e-01, 4.5263e-01],\n",
            "        [5.6314e-01, 5.7812e-01],\n",
            "        [3.9655e-01, 2.5845e-02],\n",
            "        [1.4649e-01, 5.4976e-02],\n",
            "        [3.5006e-01, 4.8683e-02],\n",
            "        [5.7590e-01, 1.8352e-01],\n",
            "        [8.6473e-02, 3.8683e-01],\n",
            "        [3.6300e-01, 9.4088e-01],\n",
            "        [9.9598e-01, 4.5847e-01],\n",
            "        [4.7288e-01, 2.2308e-01],\n",
            "        [8.2062e-01, 6.9263e-01],\n",
            "        [4.4782e-01, 4.3829e-01],\n",
            "        [4.5034e-01, 6.4574e-01],\n",
            "        [6.9995e-01, 8.0806e-01],\n",
            "        [8.9999e-01, 3.0701e-01],\n",
            "        [6.4031e-01, 1.8068e-01],\n",
            "        [6.6627e-01, 9.0838e-01],\n",
            "        [9.4845e-01, 5.3033e-01],\n",
            "        [2.1147e-01, 7.1128e-01],\n",
            "        [5.4401e-01, 1.8115e-01],\n",
            "        [2.2312e-01, 6.9118e-01],\n",
            "        [9.3540e-01, 2.1503e-01],\n",
            "        [6.5990e-02, 3.4868e-01],\n",
            "        [8.8371e-01, 7.6924e-01],\n",
            "        [3.4046e-01, 7.4666e-01],\n",
            "        [6.5709e-01, 8.3878e-01],\n",
            "        [7.4479e-01, 2.4308e-01],\n",
            "        [9.5250e-01, 2.6937e-01],\n",
            "        [3.1634e-01, 2.5896e-01],\n",
            "        [3.1996e-01, 5.7929e-01],\n",
            "        [4.2979e-01, 3.1234e-01],\n",
            "        [4.7798e-01, 4.3969e-01],\n",
            "        [4.5437e-01, 6.5093e-01],\n",
            "        [9.9283e-01, 6.6737e-01],\n",
            "        [2.2679e-01, 6.6857e-01],\n",
            "        [4.6189e-01, 1.2804e-01],\n",
            "        [6.9166e-01, 3.5594e-01],\n",
            "        [4.6544e-02, 2.4684e-01],\n",
            "        [1.2763e-01, 5.0689e-01],\n",
            "        [6.5823e-01, 3.9945e-01],\n",
            "        [5.2036e-01, 7.6673e-01],\n",
            "        [2.4865e-01, 6.6240e-01],\n",
            "        [7.1887e-01, 9.3176e-01],\n",
            "        [4.0385e-01, 8.6301e-01],\n",
            "        [4.2285e-01, 1.5781e-01],\n",
            "        [9.9743e-01, 7.6966e-01],\n",
            "        [1.5980e-01, 6.2198e-01],\n",
            "        [1.1492e-01, 8.8647e-01],\n",
            "        [2.2423e-02, 4.6048e-01],\n",
            "        [1.6413e-01, 4.7860e-01],\n",
            "        [2.6478e-01, 1.3234e-02],\n",
            "        [5.7486e-01, 4.2978e-01],\n",
            "        [6.8072e-01, 6.3870e-01],\n",
            "        [2.1202e-01, 3.3155e-01],\n",
            "        [1.5255e-01, 7.8272e-01],\n",
            "        [3.0053e-01, 3.5052e-01],\n",
            "        [9.3859e-02, 7.1367e-01],\n",
            "        [3.3379e-01, 7.7035e-02],\n",
            "        [5.3340e-02, 9.6240e-01],\n",
            "        [3.1970e-01, 1.7948e-01],\n",
            "        [4.7820e-01, 1.0837e-01],\n",
            "        [6.8105e-01, 8.2493e-01],\n",
            "        [5.4151e-01, 9.5051e-01],\n",
            "        [5.5386e-01, 4.7440e-01],\n",
            "        [5.4978e-01, 1.0431e-01],\n",
            "        [6.0508e-01, 6.3542e-01],\n",
            "        [7.8321e-01, 7.4402e-01],\n",
            "        [7.8362e-02, 6.4678e-01],\n",
            "        [6.9887e-03, 6.8872e-01],\n",
            "        [6.1168e-01, 5.2751e-01],\n",
            "        [6.9864e-01, 4.1983e-01],\n",
            "        [9.8113e-01, 2.6025e-01],\n",
            "        [7.7774e-01, 9.0437e-01],\n",
            "        [3.9987e-02, 2.7169e-01],\n",
            "        [2.0930e-02, 6.1135e-01],\n",
            "        [1.1163e-01, 9.5712e-01],\n",
            "        [1.8459e-01, 7.4458e-01],\n",
            "        [1.9736e-01, 5.6286e-01],\n",
            "        [8.7159e-01, 6.9314e-04],\n",
            "        [2.1317e-01, 6.7067e-01],\n",
            "        [3.3686e-01, 5.5169e-01],\n",
            "        [3.7093e-02, 7.0306e-01],\n",
            "        [1.3642e-01, 7.4276e-01],\n",
            "        [9.7570e-01, 2.4873e-01],\n",
            "        [4.8742e-01, 2.2932e-01],\n",
            "        [9.4234e-01, 7.8410e-01]])\n",
            "tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
            "        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,\n",
            "        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
            "        0, 1, 0, 1, 1, 0, 0, 0])\n",
            "Total loss for this batch: 0.6851\n"
          ]
        }
      ],
      "source": [
        "# Creating initial batch\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "dummy_outputs = torch.rand(batch_size_train, 2)\n",
        "dummy_labels = torch.randint(low=0, high=2, size=(batch_size_train,))\n",
        "\n",
        "print(dummy_outputs)\n",
        "print(dummy_labels)\n",
        "\n",
        "loss = loss_fn(dummy_outputs, dummy_labels)\n",
        "print('Total loss for this batch: {:.4f}'.format(loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYwvc3mIYPX0"
      },
      "outputs": [],
      "source": [
        "# # Image display function\n",
        "# def matplotlib_imshow(img, one_channel=False):\n",
        "#     if one_channel:\n",
        "#         img = img.mean(dim=0)\n",
        "#     img = img / 2 + 0.5     # unnormalize\n",
        "#     print(img)\n",
        "#     npimg = img.numpy()\n",
        "#     if one_channel:\n",
        "#         plt.imshow(npimg, cmap='Greys')\n",
        "#     else:\n",
        "#         plt.imshow(np.transpose(npimg, (1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_rML9wGFnIh"
      },
      "outputs": [],
      "source": [
        "# from sklearn.decomposition import PCA\n",
        "\n",
        "# # Get the features from the dataset\n",
        "# features = None\n",
        "# for images, labels in original_train_set:\n",
        "#     images = images.numpy()\n",
        "#     images = images.reshape(images.shape[0], -1)\n",
        "#     images -= np.mean(images, axis=0)\n",
        "#     features = images if features is None else np.vstack((features, images))\n",
        "\n",
        "# # Define the number of components for PCA\n",
        "# n_components = 3\n",
        "\n",
        "# # Perform PCA on the features\n",
        "# pca = PCA(n_components=n_components)\n",
        "# pca_result = pca.fit_transform(features)\n",
        "\n",
        "\n",
        "# pca_result = pca.fit_transform(features)\n",
        "\n",
        "# # Perform PCA on the features\n",
        "# pca = PCA(n_components=n_components)\n",
        "# pca_result = pca.fit_transform(features)\n",
        "\n",
        "\n",
        "# # Print the explained variance ratio of each PCA component\n",
        "# print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGoKossnQ5o-"
      },
      "outputs": [],
      "source": [
        "# import seaborn as sns\n",
        "\n",
        "# # Get the features from the dataset\n",
        "# features = None\n",
        "# num_images = 0;\n",
        "# labels = []\n",
        "# for images, label in original_train_set:\n",
        "#     images = images.numpy()\n",
        "#     images = images.reshape(images.shape[0], -1)\n",
        "#     features_batch = pca.transform(images)\n",
        "#     features = features_batch if features is None else np.concatenate((features, features_batch), axis=0)\n",
        "#     labels.append(label)\n",
        "#     num_images += images.shape[0]\n",
        "\n",
        "# # Create a DataFrame from the PCA results and the labels\n",
        "# df = pd.DataFrame(data=features, columns=[f\"pca-{i+1}\" for i in range(n_components)])\n",
        "# df[\"label\"] = labels[:num_images]\n",
        "\n",
        "# # Plot the first two dimensions of the PCA results\n",
        "# plt.figure(figsize=(16,10))\n",
        "# g_pca_2d = sns.scatterplot(\n",
        "#     x=\"pca-1\", y=\"pca-2\",\n",
        "#     hue=\"label\",\n",
        "#     palette = sns.color_palette(\"tab10\", n_classes),\n",
        "#     data=df,\n",
        "#     legend=\"full\",\n",
        "#     alpha=1\n",
        "# )\n",
        "# g_pca_2d.legend(loc='upper right')\n",
        "# g_pca_2d.set_title(\"First two dimensions of PCA-compressed-data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoXDG8VCzUie"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jar-SdX30e6B"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Extract features from the images using a pre-trained CNN\n",
        "# # This example uses the ResNet50 architecture\n",
        "# cnn = models.resnet50(pretrained=True)\n",
        "# cnn.eval()\n",
        "# features = []\n",
        "# labels = []\n",
        "# for img, label in original_train_set:\n",
        "#     with torch.no_grad():\n",
        "#         feature = cnn.forward(img.unsqueeze(0)).squeeze().numpy()\n",
        "#     features.append(feature)\n",
        "#     labels.append(label)\n",
        "# features = np.array(features)\n",
        "# labels = np.array(labels)\n",
        "\n",
        "# # plt.scatter(pca_result[:, 0], pca_result[:, 1],\n",
        "# #             c=digits.target, edgecolor='none', alpha=0.5,\n",
        "# #             cmap=plt.cm.get_cmap('spectral', 10))\n",
        "# # plt.xlabel('component 1')\n",
        "# # plt.ylabel('component 2')\n",
        "# plt.colorbar();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeJVw1sv79uv"
      },
      "outputs": [],
      "source": [
        "# # Create a scatter plot of the first three principal components\n",
        "# num_classes = 3\n",
        "# colors = ['green' if label == 0 else 'orange' for label in labels]\n",
        "\n",
        "# plt.scatter(pca_result[:, 0], pca_result[:, 1], c=pca_result[:, 2],\n",
        "#             edgecolor='none', alpha=0.5, cmap='spring', s=10)\n",
        "# # Set the path to the folder where you want to save the file\n",
        "# path = '/content/drive/MyDrive/My Graphs/'\n",
        "\n",
        "# if not os.path.exists(directory):\n",
        "#     os.makedirs(directory)\n",
        "\n",
        "\n",
        "# plt.xlabel('component 1')\n",
        "# plt.ylabel('component 2')\n",
        "# plt.ylabel('component 3')\n",
        "# plt.colorbar()\n",
        "\n",
        "\n",
        "# # Save the file with a specific name\n",
        "# plt.savefig(path + 'output.png')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMmR7b4TbXXN"
      },
      "outputs": [],
      "source": [
        "# dataiter = iter(train_loader)\n",
        "# images, labels = next(dataiter)\n",
        "\n",
        "# img_grid = torchvision.utils.make_grid(images)\n",
        "# print(img_grid.shape)\n",
        "# matplotlib_imshow(img_grid, one_channel=False)\n",
        "\n",
        "# print(images.shape)\n",
        "# print('  '.join(classes[labels[j]] for j in range(batch_size_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vLnYbEJfWr4"
      },
      "outputs": [],
      "source": [
        "# class my_vgg16(nn.Module):\n",
        "#     def __init__(self, originalModel, num_of_channels=3, num_classes=2):\n",
        "#         super(my_vgg16, self).__init__()\n",
        "#         self.features = nn.Sequential(*list(originalModel.features)[:])\n",
        "\n",
        "#         self.fully_connected_layers = nn.Sequential(\n",
        "#             nn.Linear(512 * 7 * 7, 4096),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(p=0.5),\n",
        "#             nn.Linear(4096, 4096),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(p=0.5),\n",
        "#             nn.Linear(4096, num_classes),\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.features(x)\n",
        "#         x = x.reshape(x.shape[0], -1)\n",
        "#         x = self.fully_connected_layers(x)\n",
        "#         return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmzUm032e2V9"
      },
      "outputs": [],
      "source": [
        "# # Build model and check its properties\n",
        "# model_vgg16 = torch.hub.load('pytorch/vision:v0.8.0', 'vgg16', pretrained=True)\n",
        "# print(model_vgg16)\n",
        "# for i, param in enumerate(model_vgg16.parameters()):\n",
        "#     param.requires_grad = False\n",
        "#     print(i)\n",
        "#     print(param.requires_grad)\n",
        "# model = my_vgg16(model_vgg16)\n",
        "# print(model)\n",
        "# for i, param in enumerate(model.parameters()):\n",
        "#     print(i)\n",
        "#     print(param.requires_grad)\n",
        "\n",
        "# # If GPU is available\n",
        "# if gpu_device == 'cuda':\n",
        "#     model.to(gpu_device)           ### GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiPxSgikFo-c"
      },
      "outputs": [],
      "source": [
        " class my_resnet50(nn.Module):\n",
        "    def __init__(self, originalModel, num_of_channels=3, num_classes=2):\n",
        "        super(my_resnet50, self).__init__()\n",
        "        self.features = nn.Sequential(*list(originalModel.children())[:-1])\n",
        "        self.fully_connected_layers = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(512, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fully_connected_layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zw0xYEqNZuGl",
        "outputId": "e249b4bc-2071-44fc-c70a-f8cebf76e891"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.8.0\" to /root/.cache/torch/hub/v0.8.0.zip\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 227MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of trainable parameters: 2624002\n",
            "Number of trainable parameters: 2624002\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load ResNet50 model\n",
        "model_resnet50 = torch.hub.load('pytorch/vision:v0.8.0', 'resnet50', pretrained=True)\n",
        "\n",
        "# Freeze all the parameters in the ResNet50 model\n",
        "for param in model_resnet50.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Create custom model by replacing the last layer with our fully connected layers\n",
        "model = my_resnet50(model_resnet50)\n",
        "\n",
        "# Set the gradients for the fully connected layers to be trainable\n",
        "for param in model.fully_connected_layers.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# If GPU is available\n",
        "# if gpu_device == 'cuda':\n",
        "    # model.to(gpu_device)\n",
        "\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Number of trainable parameters: {num_params}\")\n",
        "\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Number of trainable parameters: {num_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUfD8AmTKqYC",
        "outputId": "390d7953-413c-46b3-bd22-9d64913f7a91"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.8.0\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100%|██████████| 171M/171M [00:02<00:00, 82.0MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of trainable parameters: 12590082\n"
          ]
        }
      ],
      "source": [
        "class my_resnet101(nn.Module):\n",
        "    def __init__(self, originalModel, num_of_channels=3, num_classes=2):\n",
        "        super(my_resnet101, self).__init__()\n",
        "        self.features = nn.Sequential(*list(originalModel.children())[:-1])\n",
        "        self.fully_connected_layers = nn.Sequential(\n",
        "            nn.Linear(2048, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(4096, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(1024, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fully_connected_layers(x)\n",
        "        return x\n",
        "\n",
        "# Load ResNet101 model\n",
        "model_resnet101 = torch.hub.load('pytorch/vision:v0.8.0', 'resnet101', pretrained=True)\n",
        "\n",
        "# Freeze all the parameters in the ResNet101 model\n",
        "for param in model_resnet101.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Create custom model by replacing the last layer with our fully connected layers\n",
        "model = my_resnet101(model_resnet101)\n",
        "\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Number of trainable parameters: {num_params}\")\n",
        "\n",
        "# Set the gradients for the fully connected layers to be trainable\n",
        "for param in model.fully_connected_layers.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# If GPU is available\n",
        "# if gpu_device == 'cuda':\n",
        "    # model.to(gpu_device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s34iYmc1-snu"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "# # Optimizers initiation\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr= 0.01, momentum=0.9)\n",
        "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.8)\n",
        "\n",
        "# optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "# initialize lists to store train and validation accuracy\n",
        "train_acc_list = []\n",
        "val_acc_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZuUowKY7nze"
      },
      "outputs": [],
      "source": [
        "# One loop training function\n",
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "    running_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, labels = data\n",
        "\n",
        "        # If GPU is available\n",
        "        if gpu_device == 'cuda':\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()           ### GPU\n",
        "\n",
        "        # Zero your gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        total_samples += inputs.size(0)\n",
        "\n",
        "        if i % 12 == 1:\n",
        "            last_loss = running_loss / batch_size_train\n",
        "            accuracy = running_corrects.double() / total_samples\n",
        "            print(\"\\rbatch {}/{}, loss: {:.4f}, accuracy: {:.2%}\".format(i + 1, len(train_loader), last_loss, accuracy), end='', flush=True)\n",
        "            print(\"\\rbatch {}/{}, loss: {:.4f}\".format(i+1, len(train_loader), last_loss, end='', flush=True))\n",
        "            tb_x = epoch_index * len(train_loader) + i + 1\n",
        "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "            running_loss = 0.\n",
        "            running_corrects = 0\n",
        "            total_samples = 0\n",
        "\n",
        "    return last_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDHXVJ2DaUb3"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w87zgzwrahSX",
        "outputId": "871b5890-6562-49d9-c123-ce3f95160097"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.1-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.1)\n",
            "Collecting protobuf>=4.22.3 (from tensorboardX)\n",
            "  Downloading protobuf-4.23.3-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, tensorboardX\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "Successfully installed protobuf-4.23.3 tensorboardX-2.6.1\n"
          ]
        }
      ],
      "source": [
        "pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwCwpNElbYfi"
      },
      "outputs": [],
      "source": [
        "# Initializing TensorBoard data writer\n",
        "import datetime\n",
        "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "from tensorboardX import SummaryWriter\n",
        "writer = SummaryWriter('runs/Breast_cancer_trainer_{}'.format(timestamp))\n",
        "epoch_number = 0\n",
        "\n",
        "model_dir = '/content/drive/MyDrive/models'\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "model_path = os.path.join(model_dir, 'model_{}_{}'.format(timestamp, epoch_number))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVQy-1xgcfxc",
        "outputId": "79e73594-f203-4ef3-8bcf-f05a3145f4a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.001\n",
            "Optimizer: SGD\n",
            "Number of trainable parameters: 12590082\n",
            "EPOCH 2:\n",
            "0.002\n",
            "Loaded pre-trained model: /content/drive/MyDrive/models/model_20230624_050006_1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch 2/79, loss: 0.0034\n",
            "batch 14/79, loss: 0.0185\n",
            "batch 26/79, loss: 0.0154\n",
            "batch 38/79, loss: 0.0150\n"
          ]
        }
      ],
      "source": [
        "total_epochs = 2\n",
        "best_vloss = 1000000.\n",
        "start_epoch = 0\n",
        "epoch_number = 1\n",
        "\n",
        "# Before the loop\n",
        "all_vlabels = []\n",
        "all_vpreds = []\n",
        "all_voutputs = []\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, roc_auc_score\n",
        "\n",
        "f1_score_list = []\n",
        "precision_list = []\n",
        "auc_list = []\n",
        "\n",
        "for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = 0.001\n",
        "    print(param_group['lr'])\n",
        "\n",
        "# check if the model_path file exists, and load the weights if it does\n",
        "\n",
        "print(\"Optimizer:\", type(optimizer).__name__)\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Number of trainable parameters: {num_params}\")\n",
        "\n",
        "\n",
        "for epoch in range(total_epochs):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "\n",
        "    # Find the latest saved model in the model directory\n",
        "    model_dir = '/content/drive/MyDrive/models/'\n",
        "    model_files = os.listdir(model_dir)\n",
        "\n",
        "    model_files.sort(key=lambda x: os.path.getctime(os.path.join(model_dir, x)))\n",
        "    last_saved_model = model_files[-1]\n",
        "    model_path = os.path.join(model_dir, last_saved_model)\n",
        "    model_path = '/content/drive/MyDrive/models/model_20230624_050006_1'\n",
        "\n",
        "\n",
        "    checkpoint = torch.load(model_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = 0.002\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        print(param_group['lr'])\n",
        "        print('Loaded pre-trained model:', model_path)\n",
        "\n",
        "    # Turn gradient tracking on, and train\n",
        "    model.train(True)\n",
        "    avg_loss = train_one_epoch(epoch_number, writer)\n",
        "\n",
        "    # Turn gradient tracking off\n",
        "    model.train(False)\n",
        "\n",
        "    # Run validation\n",
        "    running_vloss = 0.0\n",
        "    running_vcorrects = 0.\n",
        "    total_vpreds = 0\n",
        "\n",
        "    for i, vdata in enumerate(val_loader):\n",
        "        vinputs, vlabels = vdata\n",
        "          # if gpu_device == 'cuda':1\n",
        "          # vinputs, vlabels = vinputs.cuda(), vlabels.cuda()\n",
        "\n",
        "        voutputs = model(vinputs)\n",
        "        vloss = loss_fn(voutputs, vlabels)\n",
        "        vpreds = torch.argmax(voutputs, dim=1)\n",
        "        running_vloss += vloss.item()\n",
        "        running_vcorrects += torch.sum(vpreds == vlabels.data)\n",
        "        total_vpreds += len(vlabels)\n",
        "        all_vlabels.append(vlabels)\n",
        "        all_vpreds.append(vpreds)\n",
        "        all_voutputs.append(voutputs)\n",
        "\n",
        "        vlabels_all = torch.cat(all_vlabels, dim=0).flatten().detach().numpy()\n",
        "        vpreds_all = torch.cat(all_vpreds, dim=0).flatten().detach().numpy()\n",
        "        concatenated_outputs = torch.cat(all_voutputs, dim=0)\n",
        "        flattened_outputs = concatenated_outputs.view(-1).detach().numpy()\n",
        "\n",
        "\n",
        "\n",
        "        print(\"vlabels shape:\", vlabels_all.shape)\n",
        "        print(\"vpreds shape:\", vpreds_all.shape)\n",
        "        print(\"outputs shape:\", flattened_outputs.shape)\n",
        "\n",
        "\n",
        "    avg_vloss = running_vloss / (i + 1)\n",
        "    avg_vacc = running_vcorrects / total_vpreds\n",
        "    print('LOSS valid {:.4f}, ACC valid {:.4f}'.format(avg_vloss, avg_vacc))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    model_path = os.path.join(model_dir, 'model_{}_{}'.format(timestamp, epoch_number))\n",
        "    torch.save({\n",
        "        'epoch': epoch_number,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'best_vloss': best_vloss,\n",
        "        'val_acc_list': val_acc_list\n",
        "    }, model_path)\n",
        "\n",
        "    f1 = f1_score(vlabels_all, vpreds_all, average='weighted')  # Use scikit-learn's f1_score\n",
        "    precision = precision_score(vlabels_all, vpreds_all, average='weighted')  # Use scikit-learn's precision_score\n",
        "\n",
        "    print('f1 : ',f1)\n",
        "\n",
        "    # Append the train and validation accuracy to the lists\n",
        "\n",
        "    val_acc_list.append(avg_vacc)\n",
        "    f1_score_list.append(f1)\n",
        "    precision_list.append(precision)\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "\n",
        "    # Update the learning rate\n",
        "    # lr_scheduler.step()\n",
        "\n",
        "\n",
        "    epoch_number += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTmEzSw357n_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfAGGsUhmuBs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgdMwvRP3pAe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# model_path = '/content/drive/MyDrive/models/model_20230319_151031_13'\n",
        "6g# checkpoint = torch.load(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjvVSoTwTJUz"
      },
      "outputs": [],
      "source": [
        "# # plot the train and validation accuracy for each epoch\n",
        "# plt.plot(train_acc_list, label='Train accuracy', color='blue', linestyle='solid')\n",
        "# plt.plot(val_acc_list, label='Validation accuracy', color='orange', linestyle='dashed')\n",
        "# # add a title,6 labels, and legend\n",
        "# plt.title('Accuracy over Epochs')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.legend()\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# train_acc_list = checkpoint.get('train_acc_list', [])\n",
        "# val_acc_list = checkpoint.get('val_acc_list', [])\n",
        "\n",
        "\n",
        "# create a figure with two subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# create a figure with a single subplot\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "# Set the path to the folder where you want to save the file\n",
        "directory = '/content/drive/MyDrive/My Graphs/'\n",
        "path = '/content/drive/MyDrive/My Graphs/neww.png'\n",
        "\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "print(f1_score_list)\n",
        "\n",
        "# plot the train and validation accuracy for each epoch as a line plot\n",
        "ax1.plot(train_acc_list, label='Train accuracy')\n",
        "ax1.plot(val_acc_list, label='Validation accuracy')\n",
        "# set the y-axis range manually\n",
        "ax1.set_ylim([0.85, 0.96])\n",
        "ax1.set_title('Accuracy over Epochs')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.legend()\n",
        "\n",
        "# plot the train and validation accuracy for each epoch as a scatter plot\n",
        "ax2.scatter(range(1, len(train_acc_list)+1), train_acc_list, label='Train accuracy', color='blue')\n",
        "ax2.scatter(range(1, len(val_acc_list)+1), val_acc_list, label='Validation accuracy', color='orange')\n",
        "ax2.set_title('Accuracy over Epochs')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "\n",
        "# adjust the layout and spacing between subplots\n",
        "fig.tight_layout()\n",
        "# Save the file with a specific name\n",
        "plt.savefig(path + 'neww.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MerNpn_pxWC6"
      },
      "outputs": [],
      "source": [
        "\n",
        "  # Assuming you have a test_loader that contains the test data\n",
        "  model_path = '/content/drive/MyDrive/models/model_20230624_050006_1'\n",
        "  checkpoint = torch.load(model_path)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  total_tpreds = 0.0\n",
        "  running_tcorrects = 0.0\n",
        "\n",
        "  # Turn gradient tracking off\n",
        "  model.train(False)\n",
        "\n",
        "  for i, tdata in enumerate(test_loader):\n",
        "        tinputs, tlabels = tdata\n",
        "        # If GPU is available\n",
        "         # if gpu_device == 'cuda':\n",
        "              ### GPU\n",
        "        toutputs = model(tinputs)\n",
        "        # vloss = loss_fn(voutputs, vlabels)\n",
        "        tpreds = torch.argmax(toutputs, dim=1)\n",
        "        # running_vloss += vloss.item()\n",
        "        running_tcorrects += torch.sum(tpreds == tlabels.data)\n",
        "        total_tpreds += len(tlabels)\n",
        "\n",
        "\n",
        "    # avg_vloss = running_vloss / (i + 1)\n",
        "  avg_tacc = running_tcorrects / total_tpreds\n",
        "  print('ACC {:.4f}'.format(avg_tacc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QiC9Y8J5jHm"
      },
      "outputs": [],
      "source": [
        "epoch_number"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1OwyY6BtQaRFgTJNGEEl9gI7CUHKgNlOu",
      "authorship_tag": "ABX9TyMNtiBWDifiNGILcWscj/pT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}